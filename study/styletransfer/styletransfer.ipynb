{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB\n",
      "800 600\n",
      "end model learn\n",
      "Iteration 0\n",
      "sum :  -10957582.0 cost:  43722207000.0\n",
      "Iteration 100\n",
      "sum :  -14603647.0 cost:  1220496400.0\n",
      "Iteration 200\n",
      "sum :  -14582676.0 cost:  586000500.0\n",
      "Iteration 300\n",
      "sum :  -14025271.0 cost:  365778080.0\n",
      "Iteration 400\n",
      "sum :  -13279223.0 cost:  260393300.0\n",
      "Iteration 500\n",
      "sum :  -12481779.0 cost:  198754400.0\n",
      "Iteration 600\n",
      "sum :  -11691618.0 cost:  158101860.0\n",
      "Iteration 700\n",
      "sum :  -10906240.0 cost:  169545740.0\n",
      "Iteration 800\n",
      "sum :  -10154760.0 cost:  111667910.0\n",
      "Iteration 900\n",
      "sum :  -9498490.0 cost:  97126120.0\n",
      "Iteration 1000\n",
      "sum :  -8832976.0 cost:  83170370.0\n",
      "Iteration 1100\n",
      "sum :  -8196020.5 cost:  82896790.0\n",
      "Iteration 1200\n",
      "sum :  -7591975.5 cost:  90737540.0\n",
      "Iteration 1300\n",
      "sum :  -7013618.0 cost:  56775010.0\n",
      "Iteration 1400\n",
      "sum :  -6482399.0 cost:  110063530.0\n",
      "Iteration 1500\n",
      "sum :  -5945026.0 cost:  48992976.0\n",
      "Iteration 1600\n",
      "sum :  -5460203.5 cost:  43705644.0\n",
      "Iteration 1700\n",
      "sum :  -4983864.5 cost:  47815668.0\n",
      "Iteration 1800\n",
      "sum :  -4541343.0 cost:  34815930.0\n",
      "Iteration 1900\n",
      "sum :  -4111193.0 cost:  129217800.0\n",
      "Iteration 2000\n",
      "sum :  -3738928.0 cost:  48427012.0\n",
      "Iteration 2100\n",
      "sum :  -3327367.0 cost:  29104552.0\n",
      "Iteration 2200\n",
      "sum :  -2926632.0 cost:  25444744.0\n",
      "Iteration 2300\n",
      "sum :  -2577469.0 cost:  23156354.0\n",
      "Iteration 2400\n",
      "sum :  -2203794.5 cost:  21575568.0\n",
      "Iteration 2500\n",
      "sum :  -1867429.5 cost:  27320706.0\n",
      "Iteration 2600\n",
      "sum :  -1550248.5 cost:  21703950.0\n",
      "Iteration 2700\n",
      "sum :  -1217776.5 cost:  21163312.0\n",
      "Iteration 2800\n",
      "sum :  -923872.0 cost:  23681514.0\n",
      "Iteration 2900\n",
      "sum :  -606511.0 cost:  16753478.0\n",
      "Iteration 3000\n",
      "sum :  -316052.0 cost:  17781062.0\n",
      "Iteration 3100\n",
      "sum :  -31991.0 cost:  13899505.0\n",
      "Iteration 3200\n",
      "sum :  238674.0 cost:  13541410.0\n",
      "Iteration 3300\n",
      "sum :  518021.5 cost:  677434700.0\n",
      "Iteration 3400\n",
      "sum :  -48088.5 cost:  34772004.0\n",
      "Iteration 3500\n",
      "sum :  299441.5 cost:  34631468.0\n",
      "Iteration 3600\n",
      "sum :  596868.5 cost:  23954900.0\n",
      "Iteration 3700\n",
      "sum :  828233.0 cost:  13175703.0\n",
      "Iteration 3800\n",
      "sum :  1041450.0 cost:  12406770.0\n",
      "Iteration 3900\n",
      "sum :  1229086.5 cost:  10307435.0\n",
      "Iteration 4000\n",
      "sum :  1402205.0 cost:  15644457.0\n",
      "Iteration 4100\n",
      "sum :  1460880.0 cost:  28729282.0\n",
      "Iteration 4200\n",
      "sum :  1610512.5 cost:  16083565.0\n",
      "Iteration 4300\n",
      "sum :  1758268.5 cost:  10154065.0\n",
      "Iteration 4400\n",
      "sum :  1883997.5 cost:  8596714.0\n",
      "Iteration 4500\n",
      "sum :  1996262.5 cost:  8255193.5\n",
      "Iteration 4600\n",
      "sum :  2098723.0 cost:  8159973.0\n",
      "Iteration 4700\n",
      "sum :  2192653.5 cost:  7430659.5\n",
      "Iteration 4800\n",
      "sum :  1969281.0 cost:  79795950.0\n",
      "Iteration 4900\n",
      "sum :  1885645.5 cost:  17184102.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementation of the \"A Neural Algorithm of Artistic Style\" in Python using\n",
    "TensorFlow.\n",
    "\n",
    "ref \"https://github.com/log0/neural-style-painting\"\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import imageio\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "\n",
    "###############################################################################\n",
    "# Constants for the image input and output.\n",
    "###############################################################################\n",
    "\n",
    "#networks loss of content : first one layer of conv,mixed and etc(block)\n",
    "#network loss of style : 4 to backword layer\n",
    "\n",
    "\n",
    "# Output folder for the images.\n",
    "OUTPUT_DIR = 'output_/'\n",
    "# Style image to use.\n",
    "# STYLE_IMAGE = 'images/starry_night.jpg'\n",
    "CONTENT_IMAGE = 'Macau.jpg'\n",
    "#CONTENT_IMAGE = 'content.png'\n",
    "# Content image to use.\n",
    "# CONTENT_IMAGE = 'images/hong_kong_2.jpg'\n",
    "#STYLE_IMAGE = 'StarryNight.jpg'\n",
    "#STYLE_IMAGE = 'style_hw2.jpg'\n",
    "STYLE_IMAGE = 'rem.jpg'\n",
    "# Image dimensions constants. \n",
    "from PIL import Image\n",
    "im = Image.open(CONTENT_IMAGE)\n",
    "print(im.mode)\n",
    "width, height = im.size\n",
    "\n",
    "if(im.mode == \"RGBA\"):\n",
    "    background = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "    background.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "\n",
    "    background.save(CONTENT_IMAGE)\n",
    "\n",
    "IMAGE_WIDTH = width\n",
    "IMAGE_HEIGHT = height\n",
    "COLOR_CHANNELS = 3\n",
    "\n",
    "print(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "im = Image.open(STYLE_IMAGE)\n",
    "im = im.resize((width, height), Image.ANTIALIAS)\n",
    "if(im.mode == \"RGBA\"):\n",
    "    background = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "    background.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "\n",
    "    background.save(STYLE_IMAGE)\n",
    "else:\n",
    "    im.save(STYLE_IMAGE)\n",
    "\n",
    "###############################################################################\n",
    "# Algorithm constants\n",
    "###############################################################################\n",
    "# Noise ratio. Percentage of weight of the noise for intermixing with the\n",
    "# content image.\n",
    "NOISE_RATIO = 0.6\n",
    "# Number of iterations to run.\n",
    "ITERATIONS = 5000\n",
    "# Constant to put more emphasis on content loss.\n",
    "# Default 5\n",
    "BETA = 5\n",
    "# Constant to put more emphasis on style loss.\n",
    "# Default 100\n",
    "ALPHA = 100\n",
    "# Path to the deep learning model. This is more than 500MB so will not be\n",
    "# included in the repository, but available to download at the model Zoo:\n",
    "# Link: https://github.com/BVLC/caffe/wiki/Model-Zoo\n",
    "#\n",
    "# Pick the VGG 19-layer model by from the paper \"Very Deep Convolutional \n",
    "# Networks for Large-Scale Image Recognition\".\n",
    "VGG_MODEL = 'imagenet-vgg-verydeep-19.mat'\n",
    "#VGG_MODEL = 'vgg-face.mat'\n",
    "# The mean to subtract from the input to the VGG model. This is the mean that\n",
    "# when the VGG was used to train. Minor changes to this will make a lot of\n",
    "# difference to the performance of model.\n",
    "MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "\n",
    "def generate_noise_image(content_image, noise_ratio = NOISE_RATIO):\n",
    "    \"\"\"\n",
    "    Returns a noise image intermixed with the content image at a certain ratio.\n",
    "    \"\"\"\n",
    "    noise_image = np.random.uniform(\n",
    "            -20, 20,\n",
    "            (1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)).astype('float32')\n",
    "    # White noise image from the content representation. Take a weighted average\n",
    "    # of the values\n",
    "    input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "    return input_image\n",
    "\n",
    "def load_image(path):\n",
    "    image = imageio.imread(path)\n",
    "    # Resize the image for convnet input, there is no change but just\n",
    "    # add an extra dimension.\n",
    "    image = np.reshape(image, ((1,) + image.shape))\n",
    "    # Input to the VGG model expects the mean to be subtracted.\n",
    "    image = image - MEAN_VALUES\n",
    "    return image\n",
    "\n",
    "def save_image(path, image):\n",
    "    # Output should add back the mean.\n",
    "    image = image + MEAN_VALUES\n",
    "    # Get rid of the first useless dimension, what remains is the image.\n",
    "    image = image[0]\n",
    "    image = np.clip(image, 0, 255).astype('uint8')\n",
    "    imageio.imsave(path, image)\n",
    "\n",
    "def load_inception_model(frozen_graph_filename):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the\n",
    "    # unserialized graph_def\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(\n",
    "            graph_def,\n",
    "            input_map=None,\n",
    "            return_elements=None,\n",
    "            name=\"prefix\",\n",
    "            op_dict=None,\n",
    "            producer_op_list=None\n",
    "        )\n",
    "    #op.values() for op in graph.get_operations()\n",
    "    network = {}\n",
    "    network['input'] = tf.Variable(np.zeros((1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)), dtype = 'float32')    \n",
    "    \n",
    "    for n in graph.get_operations():\n",
    "        print(n.name)\n",
    "        network[n.name] = n\n",
    "    return network\n",
    "\"\"\"\n",
    "def load_vgg_model(param_path):\n",
    "    data = scipy.io.loadmat(param_path)\n",
    "\n",
    "    # read meta info\n",
    "    meta = data['meta']\n",
    "    classes = meta['classes']\n",
    "    class_names = classes[0][0]['description'][0][0]\n",
    "    normalization = meta['normalization']\n",
    "    average_image = np.squeeze(normalization[0][0]['averageImage'][0][0][0][0])\n",
    "    image_size = np.squeeze(normalization[0][0]['imageSize'][0][0])\n",
    "    #input_maps = tf.image.resize_images(input_maps, image_size[0], image_size[1])\n",
    "    \n",
    "    # read layer info\n",
    "    layers = data['layers']\n",
    "    network = {}\n",
    "    network['input']   = tf.Variable(np.zeros((1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)), dtype = 'float32')\n",
    "    current = network['input']\n",
    "    for layer in layers[0]:\n",
    "        name = layer[0]['name'][0][0]\n",
    "        layer_type = layer[0]['type'][0][0]\n",
    "        if layer_type == 'conv':\n",
    "            if name[:2] == 'fc':\n",
    "                padding = 'VALID'\n",
    "            else:\n",
    "                padding = 'SAME'\n",
    "            stride = layer[0]['stride'][0][0]\n",
    "            kernel, bias = layer[0]['weights'][0][0]\n",
    "            # kernel = np.transpose(kernel, (1, 0, 2, 3))\n",
    "            bias = np.squeeze(bias).reshape(-1)\n",
    "            conv = tf.nn.conv2d(current, tf.constant(kernel),\n",
    "                                strides=(1, stride[0], stride[0], 1), padding=padding)\n",
    "            current = tf.nn.bias_add(conv, bias)\n",
    "            #print name, 'stride:', stride, 'kernel size:', np.shape(kernel)\n",
    "        elif layer_type == 'relu':\n",
    "            current = tf.nn.relu(current)\n",
    "            #print name\n",
    "        elif layer_type == 'pool':\n",
    "            stride = layer[0]['stride'][0][0]\n",
    "            pool = layer[0]['pool'][0][0]\n",
    "            current = tf.nn.max_pool(current, ksize=(1, pool[0], pool[1], 1),\n",
    "                                     strides=(1, stride[0], stride[0], 1), padding='SAME')\n",
    "            #print name, 'stride:', stride\n",
    "        elif layer_type == 'softmax':\n",
    "            current = tf.nn.softmax(tf.reshape(current, [-1, len(class_names)]))\n",
    "            #print name\n",
    "\n",
    "        print(\"layer name : \"+name)\n",
    "        network[name] = current\n",
    "\n",
    "    return network\n",
    "    \n",
    "\"\"\"\n",
    "def load_vgg_model(path):\n",
    "    '''\n",
    "    Returns a model for the purpose of 'painting' the picture.\n",
    "\n",
    "    Takes only the convolution layer weights and wrap using the TensorFlow\n",
    "    Conv2d, Relu and AveragePooling layer. VGG actually uses maxpool but\n",
    "    the paper indicates that using AveragePooling yields better results.\n",
    "    The last few fully connected layers are not used.\n",
    "\n",
    "    Here is the detailed configuration of the VGG model:\n",
    "\n",
    "        0 is conv1_1 (3, 3, 3, 64)\n",
    "        1 is relu\n",
    "        2 is conv1_2 (3, 3, 64, 64)\n",
    "        3 is relu    \n",
    "        4 is maxpool\n",
    "        5 is conv2_1 (3, 3, 64, 128)\n",
    "        6 is relu\n",
    "        7 is conv2_2 (3, 3, 128, 128)\n",
    "        8 is relu\n",
    "        9 is maxpool\n",
    "        10 is conv3_1 (3, 3, 128, 256)\n",
    "        11 is relu\n",
    "        12 is conv3_2 (3, 3, 256, 256)\n",
    "        13 is relu\n",
    "        14 is conv3_3 (3, 3, 256, 256)\n",
    "        15 is relu\n",
    "        16 is conv3_4 (3, 3, 256, 256)\n",
    "        17 is relu\n",
    "        18 is maxpool\n",
    "        19 is conv4_1 (3, 3, 256, 512)\n",
    "        20 is relu\n",
    "        21 is conv4_2 (3, 3, 512, 512)\n",
    "        22 is relu\n",
    "        23 is conv4_3 (3, 3, 512, 512)\n",
    "        24 is relu\n",
    "        25 is conv4_4 (3, 3, 512, 512)\n",
    "        26 is relu\n",
    "        27 is maxpool\n",
    "        28 is conv5_1 (3, 3, 512, 512)\n",
    "        29 is relu\n",
    "        30 is conv5_2 (3, 3, 512, 512)\n",
    "        31 is relu\n",
    "        32 is conv5_3 (3, 3, 512, 512)\n",
    "        33 is relu\n",
    "        34 is conv5_4 (3, 3, 512, 512)\n",
    "        35 is relu\n",
    "        36 is maxpool\n",
    "        37 is fullyconnected (7, 7, 512, 4096)\n",
    "        38 is relu\n",
    "        39 is fullyconnected (1, 1, 4096, 4096)\n",
    "        40 is relu\n",
    "        41 is fullyconnected (1, 1, 4096, 1000)\n",
    "        42 is softmax\n",
    "    '''\n",
    "    vgg = scipy.io.loadmat(path)\n",
    "\n",
    "    vgg_layers = vgg['layers']\n",
    "    def _weights(layer, expected_layer_name):\n",
    "        \"\"\"\n",
    "        Return the weights and bias from the VGG model for a given layer.\n",
    "        \"\"\"\n",
    "        W = vgg_layers[0][layer][0][0][0][0][0]\n",
    "        b = vgg_layers[0][layer][0][0][0][0][1]\n",
    "        layer_name = vgg_layers[0][layer][0][0][-2]\n",
    "        #assert layer_name == expected_layer_name\n",
    "        return W, b\n",
    "\n",
    "    def _relu(conv2d_layer):\n",
    "        \"\"\"\n",
    "        Return the RELU function wrapped over a TensorFlow layer. Expects a\n",
    "        Conv2d layer input.\n",
    "        \"\"\"\n",
    "        return tf.nn.relu(conv2d_layer)\n",
    "\n",
    "    def _conv2d(prev_layer, layer, layer_name):\n",
    "        \"\"\"\n",
    "        Return the Conv2D layer using the weights, biases from the VGG\n",
    "        model at 'layer'.\n",
    "        \"\"\"\n",
    "        W, b = _weights(layer, layer_name)\n",
    "        W = tf.constant(W)\n",
    "        b = tf.constant(np.reshape(b, (b.size)))\n",
    "        return tf.nn.conv2d(\n",
    "            prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b\n",
    "\n",
    "    def _conv2d_relu(prev_layer, layer, layer_name):\n",
    "        \"\"\"\n",
    "        Return the Conv2D + RELU layer using the weights, biases from the VGG\n",
    "        model at 'layer'.\n",
    "        \"\"\"\n",
    "        return _relu(_conv2d(prev_layer, layer, layer_name))\n",
    "\n",
    "    def _avgpool(prev_layer):\n",
    "        \"\"\"\n",
    "        Return the AveragePooling layer.\n",
    "        \"\"\"\n",
    "        return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Constructs the graph model.\n",
    "    graph = {}\n",
    "    graph['input']   = tf.Variable(np.zeros((1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)), dtype = 'float32')\n",
    "    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n",
    "    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n",
    "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
    "    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n",
    "    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n",
    "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
    "    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n",
    "    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n",
    "    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n",
    "    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n",
    "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
    "    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n",
    "    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n",
    "    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n",
    "    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n",
    "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
    "    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n",
    "    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n",
    "    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n",
    "    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n",
    "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
    "    return graph\n",
    "'''\n",
    "FOR VGG 19 IMAGENE\n",
    "'''\n",
    "def content_loss_func(sess, model):\n",
    "    \"\"\"\n",
    "    Content loss function as defined in the paper.\n",
    "    \"\"\"\n",
    "    def _content_loss(p, x):\n",
    "        # N is the number of filters (at layer l).\n",
    "        N = p.shape[3]\n",
    "        # M is the height times the width of the feature map (at layer l).\n",
    "        M = p.shape[1] * p.shape[2]\n",
    "        # Interestingly, the paper uses this form instead:\n",
    "        #\n",
    "        #   0.5 * tf.reduce_sum(tf.pow(x - p, 2)) \n",
    "        #\n",
    "        # But this form is very slow in \"painting\" and thus could be missing\n",
    "        # out some constants (from what I see in other source code), so I'll\n",
    "        # replicate the same normalization constant as used in style loss.\n",
    "        return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(x - p, 2))\n",
    "    return _content_loss(sess.run(model['conv4_2']), model['conv4_2'])\n",
    "\n",
    "def inception_content_loss_func(sess, model):\n",
    "    \"\"\"\n",
    "    Content loss function as defined in the paper.\n",
    "    \"\"\"\n",
    "    def _content_loss(p, x):\n",
    "        # N is the number of filters (at layer l).\n",
    "        N = p.shape[3]\n",
    "        # M is the height times the width of the feature map (at layer l).\n",
    "        M = p.shape[1] * p.shape[2]\n",
    "        # Interestingly, the paper uses this form instead:\n",
    "        #\n",
    "        #   0.5 * tf.reduce_sum(tf.pow(x - p, 2)) \n",
    "        #\n",
    "        # But this form is very slow in \"painting\" and thus could be missing\n",
    "        # out some constants (from what I see in other source code), so I'll\n",
    "        # replicate the same normalization constant as used in style loss.\n",
    "        return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(x - p, 2))\n",
    "    return _content_loss(sess.run(model['conv4_2']), model['conv4_2'])\n",
    "\n",
    "\n",
    "def style_loss_func(sess, model):\n",
    "    \"\"\"\n",
    "    Style loss function as defined in the paper.\n",
    "    \"\"\"\n",
    "    def _gram_matrix(F, N, M):\n",
    "        \"\"\"\n",
    "        The gram matrix G.\n",
    "        \"\"\"\n",
    "        Ft = tf.reshape(F, (M, N))\n",
    "        return tf.matmul(tf.transpose(Ft), Ft)\n",
    "\n",
    "    def _style_loss(a, x):\n",
    "        \"\"\"\n",
    "        The style loss calculation.\n",
    "        \"\"\"\n",
    "        # N is the number of filters (at layer l).\n",
    "        N = a.shape[3]\n",
    "        # M is the height times the width of the feature map (at layer l).\n",
    "        M = a.shape[1] * a.shape[2]\n",
    "        # A is the style representation of the original image (at layer l).\n",
    "        A = _gram_matrix(a, N, M)\n",
    "        # G is the style representation of the generated image (at layer l).\n",
    "        G = _gram_matrix(x, N, M)\n",
    "        result = (1 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2))\n",
    "        return result\n",
    "\n",
    "    # Layers to use. We will use these layers as advised in the paper.\n",
    "    # To have softer features, increase the weight of the higher layers\n",
    "    # (conv5_1) and decrease the weight of the lower layers (conv1_1).\n",
    "    # To have harder features, decrease the weight of the higher layers\n",
    "    # (conv5_1) and increase the weight of the lower layers (conv1_1).\n",
    "    layers = [\n",
    "        ('conv1_1', 0.5),\n",
    "        ('conv2_1', 1.0),\n",
    "        ('conv3_1', 1.5),\n",
    "        ('conv4_1', 3.0),\n",
    "        ('conv5_1', 4.0),\n",
    "    ]\n",
    "\n",
    "    E = [_style_loss(sess.run(model[layer_name]), model[layer_name]) for layer_name, _ in layers]\n",
    "    W = [w for _, w in layers]\n",
    "    loss = sum([W[l] * E[l] for l in range(len(layers))])\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with tf.Session() as sess:\n",
    "        # Load the images.\n",
    "        content_image = load_image(CONTENT_IMAGE)\n",
    "        style_image = load_image(STYLE_IMAGE)\n",
    "        # Load the model.\n",
    "        \n",
    "        #print('temp load of inception')\n",
    "        #load_inception_model('./20180402-114759/20180402-114759.pb')\n",
    "        #print('end of fucking thing dude')\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print('start model loding')\n",
    "        \n",
    "        \n",
    "        model = load_vgg_model(VGG_MODEL)\n",
    "\n",
    "        #print('end model loding')\n",
    "        \n",
    "        # Generate the white noise and content presentation mixed image\n",
    "        # which will be the basis for the algorithm to \"paint\".\n",
    "        input_image = generate_noise_image(content_image)\n",
    "\n",
    "        #print('start model learn')\n",
    "        \n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Construct content_loss using content_image.\n",
    "        sess.run(model['input'].assign(content_image))\n",
    "        content_loss = content_loss_func(sess, model)\n",
    "\n",
    "        # Construct style_loss using style_image.\n",
    "        sess.run(model['input'].assign(style_image))\n",
    "        style_loss = style_loss_func(sess, model)\n",
    "\n",
    "        \n",
    "        print('end model learn')\n",
    "        \n",
    "        # Instantiate equation 7 of the paper.\n",
    "        total_loss = BETA * content_loss + ALPHA * style_loss\n",
    "\n",
    "        # From the paper: jointly minimize the distance of a white noise image\n",
    "        # from the content representation of the photograph in one layer of\n",
    "        # the neywork and the style representation of the painting in a number\n",
    "        # of layers of the CNN.\n",
    "        #\n",
    "        # The content is built from one layer, while the style is from five\n",
    "        # layers. Then we minimize the total_loss, which is the equation 7.\n",
    "        optimizer = tf.train.AdamOptimizer(2.0)\n",
    "        train_step = optimizer.minimize(total_loss)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(model['input'].assign(input_image))\n",
    "        for it in range(ITERATIONS):\n",
    "            sess.run(train_step)\n",
    "            if it%100 == 0:\n",
    "                # Print every 100 iteration.\n",
    "                mixed_image = sess.run(model['input'])\n",
    "                print('Iteration %d' % (it))\n",
    "                print('sum : ', sess.run(tf.reduce_sum(mixed_image)),'cost: ', sess.run(total_loss))\n",
    "\n",
    "                if not os.path.exists(OUTPUT_DIR):\n",
    "                    os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "                filename = OUTPUT_DIR+'%d.png' % (it)\n",
    "                save_image(filename, mixed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "content loss Conv2d_2a_3x3\n",
    "\n",
    "style loss Conv2d_2a_3x3 Conv2d_4a_3x3 mixed_6a mixed_7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensour",
   "language": "python",
   "name": "tensourflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
